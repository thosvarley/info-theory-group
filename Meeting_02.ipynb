{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Meeting 2\n",
    "\n",
    "Shannon information content, entropy, joint/multivariate/residual entropy.\n",
    "\n",
    "### Readings \n",
    "\n",
    "Chapter 3, up through 3.2.1 of Bossomaier et al. (stop when you get to Mutual Information), https://link.springer.com/book/10.1007/978-3-319-43222-9\n",
    "\n",
    "\n",
    "Sections 1.1, 1.2, 1.3 of Feldman's tutorial notes.\n",
    "http://hornacek.coa.edu/dave/Tutorial/notes.pdf\n",
    "\n",
    "**\"Extra Credit\" Reading**: Simon DeDeo's *\"Information Theory for Intelligent People\"* \n",
    "http://tuvalu.santafe.edu/~simon/it.pdf\n",
    "\n",
    "### Data\n",
    "We'll be working with the same, binarized, HCP data as last time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "import seaborn as sns #Seaborn is a wrapper for matplotlib - it makes everything prettier.\n",
    "sns.set(style=\"white\")\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from scipy.stats import zscore\n",
    "\n",
    "#To save disk space, I've saved the HCP data as a compressed .npz array.\n",
    "#You can load it into the workspace with np.load(), although it cannot be directly accessed. \n",
    "data = np.load(\"data/HCP_BOLD.npz\")\n",
    "bold = zscore(data[\"signal\"], axis=1) #Z-scored for data-hygiene reasons.\n",
    "#For those that don't know: to z-score data is to subtract the mean, and divide by the standard deviation. \n",
    "#This results in data distributed around 0 w/ unit deviation. It does not make things Gaussian, but makes \n",
    "#things generally nicer.\n",
    "\n",
    "discrete = deepcopy(bold) #This is a handy utility function, for Python-people whow haven't seen it before. \n",
    "discrete[discrete > 0] = 1 #Numpy indexing at work.\n",
    "discrete[discrete < 0] = 0\n",
    "discrete = discrete.astype(\"int16\") #Use small integers if you can get away with it. Easier on RAM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premise:\n",
    "Information Theory is about:\n",
    "\n",
    "1) Quantifying your uncertainty about the world. \n",
    "\n",
    "2) Quantifying how much \"work\" you need to do to reduce your uncertainty. \n",
    "\n",
    "\n",
    "### Surprise\n",
    "\n",
    "Surprise is (one) way to quantify our uncertainty about the future. If a system is highly predictable (we have a high degree of uncertainty), when we observe it, it's state is rarely surprisng. If a usually-predictable system does something very unlikely, we are quite surprised. \n",
    "\n",
    "At it's most basic level, \"surprise\" measures exactly that: how surprised are you to see our random variable in a particular state. We can think of some basic properties we might want it to have: \n",
    "- It should be 0 if you knew what the state would be with total certainty. \n",
    "- It should be inversely related to probability (i.e. the less probable a result is, the more surprising it is).\n",
    "- It would be nice if it behaved in reasonable ways (not blowing up rapidly to $\\infty$ or shrinking down to something like $10^{-50}$). \n",
    "- We would like it if, for two independent events, the surprise at seeing both together was equal to the sum of their individual surprises. \n",
    "\n",
    "\\begin{equation}\n",
    "S(X=x, Y=y) = S(X=x)+S(Y=y)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The intuition here is that, if I flip two coins at the same time and get $Heads, Heads$, that should not be any more surprising than if I flip one coin and get $Heads$ and then flip the same coin again and get $Heads$ again.\n",
    "\n",
    "Conviently, Claude Shannon provided a surprise function one for us:\n",
    "\n",
    "\\begin{equation}\n",
    "S(X=x) = \\log_2\\big(\\frac{1}{P(X=x)}\\big)\n",
    "\\end{equation}\n",
    "\n",
    "If we work through the same contrived example, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log_2(\\frac{1}{1/4}) = \\log_2(\\frac{1}{1/2}) + \\log_2(\\frac{1}{1/2}) = 2\n",
    "\\end{equation}\n",
    "\n",
    "Here's a general proof that the formula $\\log(\\frac{1}{P(X=x)})$ satisfies our additivity criteria for independent events. I'm going through the algebra in more detail than you usually see, in case we have readers who don't quite remember their rules for logarithms.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "S_{XY}(x,y) &=& \\log\\big(\\frac{1}{P_{XY}(x,y)}\\big) \\\\\n",
    "&=& -\\log(P_{XY}(x,y)) \\\\\n",
    "&=& -\\log(P_X(x)P_Y(y)) \\\\\n",
    "&=& -1\\times(\\log(P_X(x)) + \\log(P_Y(y))) \\\\\n",
    "&=& -\\log(P_X(x)) - \\log(P_Y(y))) \\\\\n",
    "&=& \\log\\big(\\frac{1}{P_X(x)}\\big) + \\log\\big(\\frac{1}{P_Y(y)}\\big) \n",
    "\\end{eqnarray}\n",
    "\n",
    "If you're looking at the above algebraic derivation thinking \"holy crap I have no idea what's happenign here\", go back and review the rules for logarithms. \n",
    "\n",
    "Some quick plots so what we can see what the surprise function looks like. Notice that, in addition to satisfying the additivity criteria, the log-based surprise also remains (reasonably) large even as the probability goes to 1, which makes computation much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon Entropy\n",
    "\n",
    "Moving on from conditional probability (which was the final \"atom\" of probability theory we needed), now we have all tools required to really dig into entropy. There are three ways that entropy of a single variable (the Shannon entropy) is commonly intuitively understood, all are equivalent, mathematically, and it is worth considering how these three definitions relate to each-other. \n",
    "\n",
    "1) **Entropy as the expected surprise.** Recall that the expected value of a distribution $P(X)$ under some value function $f(x)$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "E[P(X)] = \\sum_{x\\in\\mathcal{X}} P(X=x)f(X=x)\n",
    "\\end{equation}\n",
    "\n",
    "When our value function is the \"surprisal\" (also called the Shannon information content) given by $\\log\\big(\\frac{1}{P(X=x)}\\big)$, then we define the entropy of a distribution $H(X)$ as:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(X) &=& \\sum_{x\\in\\mathcal{X}}P(X=x)\\log\\big(\\frac{1}{P(X=x)}\\big) \\\\\n",
    "&=& -\\sum_{x\\in\\mathcal{X}}P(X=x)\\log\\big(P(X=x)\\big)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Note that, while $\\log(0)$ and $0\\times\\log(0)$ are technically undefined, by convention, $0\\times\\log(0)=0$ in information theory. Plot the fuction $f(x)=x\\log(x)$ to see why that is not totally innapropriate. The intuition is that this ensures that elements outside of $\\mathcal{X}$ have no effect on the information-structure of the variable.\n",
    "\n",
    "Typically the lower form is what is seen most commonly, although they are identical. This perspective defines entropy as a measure of, on average, how surprising is every individual possible outcome of $X$. Conviently, entropy is *always positive.* In general, if you have gotten a negative value of $H(X)$, you have done something terribly wrong. Negative information values (almost) never make sense.\n",
    "\n",
    "2) **Entropy as the average number of yes/no quesitons needed to determine the value of a variable.** The intuition for this is reasonably clear. Imagine some variable $X$ with the following probability distribution: $P(X) := \\{P(X=x_1)=1/2, P(X=x_2)=1/4, P(X=x_3)=1/4\\}$. If you were playing a game where you knew that distribution, but not the actual value of $X$ and wanted to guess the state of $X$ using the fewest number of yes/no quesitons, how would you play? \n",
    "\n",
    "You'd start by asking \"is $X=x_1$?\" Half the time, you'd be right, and the game would be over. \n",
    "\n",
    "If it's not $x_1$, then the obvious next question to ask is \"is $X=x_2$?\" After all, if it's not $x_1$, then it's just as likely to be $x_2$ as it is to be $x_3$. \n",
    "\n",
    "If it turns out you're wrong about it being $x_2$, you automatically know it's $x_3$ and the game is over. If we calculate the average number of questions asked, it works out to be:\n",
    "\n",
    "\\begin{equation}\n",
    "H(X) = \\frac{1}{2}(1) + \\frac{1}{2}(2) = 1.5\\text{ bit}\n",
    "\\end{equation}\n",
    "\n",
    "If we plug the distribution above into $-\\sum (P(X)\\log(P(X))$ above, we get the same number out.\n",
    "\n",
    "3) **Entropy as uncertainty about the state of a variable.** Finally, the intuition most commonly used in Complex Systems is that entropy quantifies how uncertain we about the state of the variable (or system) under study. As Feldman notes, Shannon's entropy function satisfies a number of intuitions. Namely that: when the state of the system is known with certainty, the entropy should be $0$ (i.e. there is no uncertainty) and when the probability distribution of states is flat, entropy is maximal (i.e. we have no reason to expect any state over any other). Furthermore, we would like the entropy is invariant under different groupings of states (again, see Feldman 1.2.1 for an example), and that the function itself is continous (no weird asymptotes, small changes in $P(X)$ lead onlly to small changes in $H(X)$).\n",
    "\n",
    "You can prove (although we won't here) that the *only* function that satisfies those four criteria is $H(X) = -\\sum P(X)\\log(P(X))$.\n",
    "\n",
    "### Joint Entropy \n",
    "\n",
    "As with probabilities, we can quantify how much uncertainty is contained within two variables, and like the other information theory measures, it is typically given as the expectation over a distribution of (joint) states (although the other two interpretations are also equally valid).\n",
    "\n",
    "\\begin{equation}\n",
    "H(X,Y) = \\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}}P(X=x, Y=y)\\log(P(X=x,Y=y))\n",
    "\\end{equation}\n",
    "\n",
    "The joint entropy generalizes to multivariate systems quite nicely:\n",
    "\n",
    "\\begin{equation}\n",
    "H(X_1,...X_n) = -\\sum_{x_1\\in\\mathcal{X_1}}...\\sum_{x_n\\in\\mathcal{X_n}}P(x_1...x_n)\\log(P(x_1...x_n))\n",
    "\\end{equation}\n",
    "\n",
    "(Note we have compactified the notation here, which we will do from here on out). \n",
    "You can also look at the joint states of groups of variables, in which case we often concatenate them without commas. For example, for four variables, we might be interested in the joint states of {X,Y} and {A,B}, which we would write $H(XY,AB)$.\n",
    "\n",
    "We have already briefly explored constructing joint probability spaces in the last Notebook, but will revisit the essential logic here, again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(Ch0,Ch1) = 1.975\n"
     ]
    }
   ],
   "source": [
    "#These have already been defined above - showing it again just for completeness sake. \n",
    "C_joint = Counter(list(zip(ch0, ch1)))\n",
    "probs_joint = {key : C_joint[key] / sum(C_joint.values()) for key in C_joint.keys()}\n",
    "\n",
    "H_joint = -1*sum((probs_joint[key]*np.log2(probs_joint[key]) for key in probs_joint.keys()))\n",
    "print(\"H(Ch0,Ch1) =\", round(H_joint, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that the joint entropy is slightly lower than what we would expect if both channels were totally random (i.e. $P(Ch 0 = 0) = P(Ch 0 = 1) = 1/2$) and both were independent. This highlights a nice property of joint entropy: the joint entropy will always be *less* that the sum of the individual entropies togther, but greater than any single consituent entropy considered alone. \n",
    "\n",
    "The logic is the same if we include more channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(0, 0, 0, 0): 180,\n",
       "         (0, 0, 0, 1): 48,\n",
       "         (0, 0, 1, 0): 76,\n",
       "         (0, 0, 1, 1): 30,\n",
       "         (0, 1, 0, 0): 105,\n",
       "         (0, 1, 0, 1): 76,\n",
       "         (0, 1, 1, 0): 18,\n",
       "         (0, 1, 1, 1): 30,\n",
       "         (1, 0, 0, 0): 47,\n",
       "         (1, 0, 0, 1): 23,\n",
       "         (1, 0, 1, 0): 100,\n",
       "         (1, 0, 1, 1): 96,\n",
       "         (1, 1, 0, 0): 32,\n",
       "         (1, 1, 0, 1): 90,\n",
       "         (1, 1, 1, 0): 38,\n",
       "         (1, 1, 1, 1): 211})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch2, ch3 = discrete[2], discrete[3]\n",
    "C_joint = Counter(list(zip(ch0, ch1, ch2, ch3)))\n",
    "C_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(Ch0,Ch1,Ch2,Ch3) = 3.281\n"
     ]
    }
   ],
   "source": [
    "probs_joint = {key : C_joint[key] / sum(C_joint.values()) for key in C_joint.keys()}\n",
    "\n",
    "H_joint = -1*sum({probs_joint[key]*np.log2(probs_joint[key]) for key in probs_joint.keys()})\n",
    "print(\"H(Ch0,Ch1,Ch2,Ch3) =\", round(H_joint, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Entropy\n",
    "\n",
    "Conditional entropy is the uncertainty you have about a variable $X$, after accounting for the states of variable $Y$. It is very important that you remember that entropy is *uncertainty* - you can think of the conditional entropy as the uncertainty in $X$ that is intrinsic to it, or not explicable using information about the state of $Y$.\n",
    "\n",
    "Typically denoted as:\n",
    "\n",
    "\\begin{equation}\n",
    "H(X | Y) = -\\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}}P(X,Y)\\log(P(X|Y))\n",
    "\\end{equation}\n",
    "\n",
    "This formula may look a little surprising (we are used to things for the form $P(X)f(P(X))$, but here we have a conditional probability on the inside of the log and and joint probability outside of it). This has to do with needing to condition all possible states of $X$ on all possible states of $Y$. If we are only interested in one possible state of $Y$, then the restricted conditional entropy is\n",
    "\n",
    "\\begin{equation}\n",
    "H(X | Y=y) = -\\sum_{x\\in\\mathcal{X}}P(X|Y=y)\\log(X|Y=y)\n",
    "\\end{equation}\n",
    "\n",
    "Which is in the more familiar shape of an entropy measure. Unfortunately, this only accounts for the effect of a specific state $y$. If we want information from all possible states of $Y$, we need to average over the distribution of $Y$'s states, which becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "H(X | Y) = \\sum_{y\\in\\mathcal{Y}}P(Y=y)H(X|Y=y)\n",
    "\\end{equation}\n",
    "\n",
    "What we have here, then is nested expectations: the expected entropy of $X$ given some static $Y=y$, and then the expected value of *that* over all possible static $Y=y$ states. This also explains why the equation immediatley above doesn't have the negative sign in front of it: that negative is \"hidden\" in $H(X|Y=y)$.\n",
    "\n",
    "When you work out the math, you get the original formulation given above. \n",
    "\n",
    "Conditional entropy can also be written in terms of joint and individual entropies:\n",
    "\n",
    "\\begin{equation}\n",
    "H(X|Y) = H(X,Y) - H(Y)\n",
    "\\end{equation}\n",
    "\n",
    "Intuitively, this makes sense: the uncertainty about $X$ conditioned on $Y$ is the amount of uncertainty *left over* when you remove the uncertainty from $Y$ from the joint states of both. Again, it is important to remember that entropy is a measure of *uncertainty* NOT *information.* There is a tendency for students to read $H(X|Y)$ as \"the information that $Y$ gives me about $X$. This is NOT the case. It is the uncertainty about $X$ left over when information about the state $Y$ is accounted for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9972551344471869\n",
      "1.0\n",
      "H(Ch0 | Ch1) =  0.9749626957207407\n",
      "H(Ch1 | Ch0) =  0.9777075612735537\n"
     ]
    }
   ],
   "source": [
    "#We can easily calculate the entropy of Ch. 0 given Ch. 1 and vice versa:\n",
    "C0 = Counter(ch0)\n",
    "C1 = Counter(ch1)\n",
    "\n",
    "probs_0 = {key : C0[key] / sum(C0.values()) for key in C0.keys()}\n",
    "probs_1 = {key : C1[key] / sum(C1.values()) for key in C1.keys()}\n",
    "\n",
    "C_joint = C_joint = Counter(list(zip(ch0, ch1))) #Again, this is already definiedm re-doing for completeness. \n",
    "probs_joint = {key : C_joint[key] / sum(C_joint.values()) for key in C_joint.keys()}\n",
    "\n",
    "H_0 = -1*sum((probs_0[key]*np.log2(probs_0[key]) for key in probs_0.keys()))\n",
    "H_1 = -1*sum((probs_1[key]*np.log2(probs_1[key]) for key in probs_1.keys()))\n",
    "\n",
    "print(H_0)\n",
    "print(H_1)\n",
    "\n",
    "H_joint = -sum((probs_joint[key]*np.log2(probs_joint[key]) for key in probs_joint.keys()))\n",
    "\n",
    "print(\"H(Ch0 | Ch1) = \", H_joint - H_1)\n",
    "print(\"H(Ch1 | Ch0) = \", H_joint - H_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these are *not* the same. The conditional entropy function is not symmetrical. Also the condtional probability $H(X|Y)$ will always be smaller than, or equal to $H(X)$, with equality if and only if $X$ and $Y$ are independenet. There is no circumstance in which knowing more about one variable decreases you ability to make predictions about the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Entropy (Kullback-Leibler Divergence)\n",
    "\n",
    "The relative entropy (or KLD) is a measure of how different two probability distributions (typically noted $P(X)$ and $Q(X)$).\n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}(P || Q) = \\sum_{x\\in\\mathcal{X}}P(X)\\log\\big(\\frac{P(X)}{Q(X)}\\big)\n",
    "\\end{equation}\n",
    "\n",
    "In the context of Bayesian inference, $D_{KL}(P||Q)$ gives the amount of information (or predictive power) gained when one updates one's prior beliefs ($Q(X)$) to the posterior distribution ($P(X)$). Alternately, it can be thought of as the amount of information lost when $Q(X)$ is used to approximate $P(X)$. \n",
    "\n",
    "Consider two distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCwAAAFcCAYAAAADCC/2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px = np.random.randint(1,101,5)\n",
    "px = px / np.sum(px)\n",
    "qx = np.repeat(1/5, 5)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.bar(np.arange(5), px)\n",
    "plt.ylim([0,1])\n",
    "plt.title(\"P(X)\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.bar(np.arange(5), qx)\n",
    "plt.ylim([0,1])\n",
    "plt.title(\"Q(X)\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.bar(np.arange(5), np.log2(px/qx))\n",
    "plt.title(\"log(P(X)/Q(X))\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Local KLD\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the element-wise differences between $Q(X=x)$ and $P(X=x)$.\n",
    "\n",
    "Working with the Shannon information content, we get $[-\\log(Q(X=x))] - [-\\log(P(X=x))]$. We subtract P(X) from Q(X) to get the \"error\" between the two distributions: how far is distribution $Q$ from distribution $P$? \n",
    "\n",
    "which can be factored to $-1(\\log(Q(X=x)) - \\log(P(X=x))$. \n",
    "\n",
    "The properties of the logarithm allow us to rewrite as $-1\\log\\big(\\frac{(Q(X=x))}{(P(X=x))}\\big)$\n",
    "\n",
    "Finally, we rewrite as $\\log\\big(\\frac{(P(X=x))}{(Q(X=x))}\\big)$\n",
    "\n",
    "Like other entropy measures, the DKL is an expectation value, over $P(X)$ (\"the posteriror\" in this context).\n",
    "\n",
    "One commonly-seen interpretation of the DKL is that it defines some kind of \"distance\" from $Q(X)$ to $P(X)$, however this is innapropriate, as the DKL doesn't define a true distance metric. It is true that $D_{KL}(P || Q) = 0$ if and only if $P(X) = Q(X)$, the measure itself is often non-symmetrical (i.e. $D_{KL}(P||Q) \\not= D_{KL}(Q||P)$! *This means that it is very important that you select your reference distribution with care.*\n",
    "\n",
    "Furthermore, there are restrictions on which distributions are valid entries for DKL. Specifically, $Q(X=x)$ can only equal $0$ if and only if $P(X=x)$ *also equals zero*, otherwise it is undefined (when both are zero, $x \\not\\in\\mathcal{X}$ and so the term is ignored, or considered to be equal to $0$).\n",
    "\n",
    "For a good video building intuition with a simple example (from a source-coding perspective, see: https://www.youtube.com/watch?v=LJwtEaP2xKA)\n",
    "\n",
    "##### Jensen-Shannon Divergence\n",
    "\n",
    "If, for whatever reason, you want a symmetric divergence between two probability distributions, the Jensen-Shannon divergence ($D_{JS}$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "D_{JS}(P||Q) = \\frac{D_{KL}(P||M)+D_{KL}(Q||M)}{2}\n",
    "\\end{equation}\n",
    "\n",
    "where $M$ is the *mixture distribuiton* of $P(X)$ and $Q(X)$,\n",
    "\n",
    "\\begin{equation}\n",
    "M=\\frac{P + Q}{2}\n",
    "\\end{equation}\n",
    "\n",
    "From these moving parts (Shannon entropy, joint entropy, conditional entropy, and KLD), it is possible to build up essentailly every subsequent bit of information theory we will cover going forward, up until we get to partial-information decomposition at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Entropy \n",
    "\n",
    "Residual entropy ($R(X)$) is a less-often used measure that (in my opinion) is extremely powerful and deserves more attention. It is also useful as a demonstration of how the various \"atomic\" entropies (Shannon, joint, condition) can be used to construct more complex measures that can provide deep insights into the structure of complex systems. \n",
    "\n",
    "The residual entropy is a measure of *how much uncertainty is intrinsic to a single element in a complex system.* For a multivariate system of interacting elements $\\textbf{X} = \\{X_1,X_2,...X_n\\}$\n",
    "\n",
    "\\begin{equation}\n",
    "R(X_i) = H(X_i | \\textbf{X}^{-i}) \n",
    "\\end{equation}\n",
    "\n",
    "Where $X^{-i} = \\{X_1, X_2,...X_{i-1}, X_{i+1},...X_n\\}$: that is to say, it is the joint-states of every element of $\\textbf{X}$ *other than $X_i$*. The residual entropy is then understood as the uncertainty in $X_i$ that cannot be resolved by observing any other elements (or combinations of elements) in the system $\\textbf{X}$.\n",
    "\n",
    "To calculate $R(X_i)$, recall that the conditional information can be decomposed into the joint and individual entropies of it's constituents. In this case, this will be two joint entropies: \n",
    "\n",
    "\\begin{equation}\n",
    "H(X_i|X^{-i}) = H(X_i,X^{-i}) - H(X^{-i}) \n",
    "\\end{equation}\n",
    "\n",
    "Obviously, $H(X_i,X^{-i}) = H(\\textbf{X})$ and $H(X^{-i})$ is the joint-entropy of every element excluding $X_i$.\n",
    "\n",
    "We can construct and calculate a simple example using the four channels selected earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual entropy of Ch.0: 0.79908896364954\n",
      "Shannon entropy of Ch. 0 0.9972551344471869\n"
     ]
    }
   ],
   "source": [
    "ch0, ch1, ch2, ch3 = discrete[0], discrete[1], discrete[2], discrete[3]\n",
    "C_full = Counter(zip(ch0, ch1, ch2, ch3))\n",
    "probs_full = {key:C_full[key] / sum(C_full.values()) for key in C_full.keys()}\n",
    "\n",
    "#Suppose we want R(ch0) - we need the joint entropy of Ch. 1, 2, and 3.\n",
    "\n",
    "C_123 = Counter(zip(ch1, ch2, ch3))\n",
    "probs_123 = {key:C_123[key] / sum(C_123.values()) for key in C_123.keys()}\n",
    "\n",
    "H_full = -sum((probs_full[key]*np.log2(probs_full[key]) for key in probs_full.keys()))\n",
    "H_123 = -sum((probs_123[key]*np.log2(probs_123[key]) for key in probs_123.keys()))\n",
    "\n",
    "print(\"Residual entropy of Ch.0:\", H_full - H_123)\n",
    "print(\"Shannon entropy of Ch. 0\", H_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are $\\approx 0.8\\text{ bit}$ of structure in Ch. 0 that cannot be extracted by observing Ch. 1-3. That's about $80\\%$ of the entropy of the channel itself, which suggests that it is not super-constrainted by these three other elements.\n",
    "\n",
    "### Scipy.Stats' Entropy Function\n",
    "\n",
    "This is the last thing. Up until now, I've been calculate entropies \"manually\" using tuple and list comprehensions, so that you can see the structure of $P\\times\\log(P)$. You don't have to do this however: the Scipy.stats package has an entropy function that you can call on distributions (it will also do the KL-divergence if you feed it two disributions of equal length).\n",
    "\n",
    "Read the docs here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html\n",
    "\n",
    "One thing to do aware of is that the default base for all the logarithms is $e$ (which outputs results in units of \"nats\", which are useful when combing information theory and thermodynamics). If you want your result in bits, make sure you specify the argument \"base=2\".\n",
    "\n",
    "If you feed it an unnormalized distribution (i.e. the values don't all sum to 1), it will normalize it for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (unnormed) [10 10 10 10 10 10 10 10 10 10]\n",
      "Entropy Function, Unnormed: 3.322 bit\n",
      " \n",
      "X_norm [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "Entropy Function, Normed 3.322 bit\n",
      " \n",
      "Entropy of X_norm, Default Base 2.303 nat\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "X = np.repeat(10, 10) #A vector of 10 10s. This is *not* a probability distribution, and should be normalized. \n",
    "#Scipy.stats entropy function doesn't care.\n",
    "print(\"X (unnormed)\", X)\n",
    "print(\"Entropy Function, Unnormed:\", round(entropy(X, base=2),3), \"bit\")\n",
    "print(\" \")\n",
    "#If we normalize it, we get the maximum entropy distribution for a 10-state system.\n",
    "X_norm = X / np.sum(X)\n",
    "print(\"X_norm\", X_norm)\n",
    "print(\"Entropy Function, Normed\", round(entropy(X_norm, base=2),3), \"bit\")\n",
    "print(\" \")\n",
    "#Finally, note that if you forget to specify the base, the numbers can be surprising. \n",
    "print(\"Entropy of X_norm, Default Base\", round(entropy(X_norm),3), \"nat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you are doing something that involves many calls to the entropy function and you forget to specify one of the bases, you can end up with bizzare things like $H(X|Y) > H(X)$. Be extra-careful with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
