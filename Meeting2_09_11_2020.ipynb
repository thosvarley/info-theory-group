{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Meeting 2\n",
    "\n",
    "Conditional probability, Bayes' rule, entropy (classical, joint, conditional, residual, relative)\n",
    "\n",
    "### Readings \n",
    "\n",
    "Chapter 3, up through 3.2.1 of Bossomaier et al. (stop when you get to Mutual Information), https://link.springer.com/book/10.1007/978-3-319-43222-9\n",
    "\n",
    "\n",
    "Sections 1.1, 1.2, 1.3 of Feldman's tutorial notes.\n",
    "http://hornacek.coa.edu/dave/Tutorial/notes.pdf\n",
    "\n",
    "**\"Extra Credit\" Reading**: Simon DeDeo's *\"Information Theory for Intelligent People\"* \n",
    "http://tuvalu.santafe.edu/~simon/it.pdf\n",
    "\n",
    "### Data\n",
    "We'll be working with the same, binarized, HCP data as last time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "import seaborn as sns #Seaborn is a wrapper for matplotlib - it makes everything prettier.\n",
    "sns.set(style=\"white\")\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from scipy.stats import zscore\n",
    "\n",
    "#To save disk space, I've saved the HCP data as a compressed .npz array.\n",
    "#You can load it into the workspace with np.load(), although it cannot be directly accessed. \n",
    "data = np.load(\"data/HCP_BOLD.npz\")\n",
    "bold = zscore(data[\"signal\"], axis=1) #Z-scored for data-hygiene reasons.\n",
    "#For those that don't know: to z-score data is to subtract the mean, and divide by the standard deviation. \n",
    "#This results in data distributed around 0 w/ unit deviation. It does not make things Gaussian, but makes \n",
    "#things generally nicer.\n",
    "\n",
    "discrete = deepcopy(bold) #This is a handy utility function, for Python-people whow haven't seen it before. \n",
    "discrete[discrete > 0] = 1 #Numpy indexing at work.\n",
    "discrete[discrete < 0] = 0\n",
    "discrete = discrete.astype(\"int16\") #Use small integers if you can get away with it. Easier on RAM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probability\n",
    "\n",
    "Along with individual and joint probabilities, condition probability is one of the basic building-blocks of probability theory. For two variables $X$ and $Y$, with support sets $\\mathcal{X},\\mathcal{Y}$, the condition probability $P(X = x | Y = y)$ is read as \"the probability that variable $X$ is in some state $x$, given that you know that varaible $Y$ is simultaniously in state $y$.\" Conditional probability can be related to the joint probability by:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(X,Y) = P(X | Y)P(Y) = P(Y | X)P(X)\n",
    "\\end{equation}$ \n",
    "\n",
    "This allows a handy derivation that:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(X | Y) = \\frac{P(X,Y)}{P(Y)}\n",
    "\\end{equation}$\n",
    "\n",
    "If and only if $X$ and $Y$ are independent can we say that:\n",
    "\n",
    "$\\begin{equation}\n",
    "P(X|Y) = P(X)\n",
    "\\end{equation}$\n",
    "\n",
    "As with the case that, for independent variables, the join probability is equal to the product of the marginals, *you should not always assume that this relationship holds. In fact, in complex systems of multiple interacting variables, it almost never does!*\n",
    "\n",
    "Conditional probability is one of the places whe the philosophy of probability gets interesting. A frequentist would argue that conditional probability is just about the number of times different combinations of $X$ and $Y$ occur together, over infinite time. When we actually want to estimate conditional probabilities from data, this is often the logic we feed the computer. However, a more philosphical/cognitive (and quasi-Bayesian) perspective is that the conditional probability hinges, not on anything intrinsic to $X$ or $Y$, but rather your *knowledge* about the state of $Y$. Intuitively, we tend to feel that the *actual* probability of $X=x$ should independent of the state of our knowledge or beliefs (what Bayesians call *priors*), however, depending on how you fall on the frequentist/Bayesian divide, you may be tempted to believe that the value $P(X=x)$ does not exist in any ontological sense, and instead is only ever a function of the state we believe the rest of the world to be in.\n",
    "\n",
    "#### Bayes' Theorem\n",
    "\n",
    "The relationship between the joint, marginal and independent probabilites allows us to derive Bayes Theorem - something every aspiring complex systems scientist should be deeply familiar with. The derivaiton is simple and hinges on the symmetrical nature of the joint probability:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "P(X,Y) = P(X|Y)P(Y) \\\\\n",
    "P(X|Y)P(Y) = P(Y|X)P(X) \\\\\n",
    "P(X|Y) = \\frac{P(Y|X)P(X)}{P(Y)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "This can lead to some non-intuitive (but profoundly important results). I won't go through an entire derivation, but one example is medical testing. Medical tests are often advertised as being \"95% accurate!\" which many interpret to mean: \"if this test comes back positive, then there's a 95% chance I have the disease.\" Formally, what they think they're getting is:\n",
    "\n",
    "$P(Disease = True | Test = +)$\n",
    "\n",
    "What the advertisement really means, however is that the test has a low false-negative rate, i.e.:\n",
    "\n",
    "$P(Test = + | Disease = True)$ \n",
    "\n",
    "These are obviously *not* identical, but Bayes rule gives us a way to calculate the information you really want, provided you can figure out what the probability of having the disease in the first place is (i.e. $P(Disease = True)$), and the probability of ever seeing a positive test results (i.e. $P(Test = +)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional probability demo\n",
    "\n",
    "We'll take two timeseries from our discrete dataset and calculate:\n",
    "\n",
    "$P(X_0 = 0)$, $P(X_0 = 1)$, as well as $P(X_0 = 0 | X_1 = 0)$ and $P(X_0 = 1 | X_1 = 0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Ch_0 = 0) 0.469\n",
      "P(Ch_0 = 1) 0.531\n"
     ]
    }
   ],
   "source": [
    "ch0 = discrete[0]\n",
    "ch1 = discrete[1]\n",
    "\n",
    "#Calculating the individual probabilities is easy. Just count how many times each state appears\n",
    "#and divide by the number of observations (in this case, the total number of frames).\n",
    "C0 = Counter(ch0)\n",
    "C1 = Counter(ch1)\n",
    "print(\"P(Ch_0 = 0)\", round(C0[0] / ch0.shape[0], 3)) #round() is a handy Python function.\n",
    "print(\"P(Ch_0 = 1)\", round(C0[1] / ch0.shape[0], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Ch_0 = 0 | Ch_1 = 0): 0.5566666667\n",
      "P(Ch_0 = 0 | Ch_1 = 1): 0.3816666667\n"
     ]
    }
   ],
   "source": [
    "#Recall from above that P(X|Y) = P(X,Y)/P(Y):\n",
    "#This requires constructing the joint probability distribution for Ch0 and Ch1\n",
    "joint = list(zip(ch0, ch1))\n",
    "C_joint = Counter(joint)\n",
    "probs_joint = {key : C_joint[key] / sum(C_joint.values()) for key in C_joint.keys()}\n",
    "\n",
    "#For readability, we will also turn the counts in C0 and C1 into probabilities. \n",
    "probs_0 = {key : C0[key] / sum(C0.values()) for key in C0.keys()}\n",
    "probs_1 = {key : C1[key] / sum(C1.values()) for key in C1.keys()}\n",
    "\n",
    "print(\"P(Ch_0 = 0 | Ch_1 = 0):\", round(probs_joint[(0,0)] / probs_1[0], 10))\n",
    "print(\"P(Ch_0 = 0 | Ch_1 = 1):\", round(probs_joint[(0,1)] / probs_1[1], 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these don't necessarilly have to sum to 1 (look up the Law of Total Probability for more on this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shannon Entropy\n",
    "\n",
    "Moving on from conditional probability (which was the final \"atom\" of probability theory we needed), now we have all tools required to really dig into entropy. There are three ways that entropy of a single variable (the Shannon entropy) is commonly intuitively understood, all are equivalent, mathematically, and it is worth considering how these three definitions relate to each-other. \n",
    "\n",
    "1) **Entropy as the expected surprise.** Recall that the expected value of a distribution $P(X)$ under some value function $f(x)$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "E[P(X)] = \\sum_{x\\in\\mathcal{X}} P(X=x)f(X=x)\n",
    "\\end{equation}\n",
    "\n",
    "When our value function is the \"surprisal\" (also called the Shannon information content) given by $\\log\\big(\\frac{1}{P(X)}\\big)$, then we define the entropy of a distribution $H(X)$ as:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(X) &=& \\sum_{x\\in\\mathcal{X}}P(X=x)\\log\\big(\\frac{1}{P(X=x)}\\big) \\\\\n",
    "&=& -\\sum_{x\\in\\mathcal{X}}P(X=x)\\log\\big(P(X=x)\\big)\n",
    "\\end{eqnarray}\n",
    "\n",
    "Note that, while $\\log(0)$ and $0\\times\\log(0)$ are technically undefined, by convention, $0\\times\\log(0)=0$ in information theory. Plot the fuction $f(x)=x\\log(x)$ to see why that is not totally innapropriate. The intuition is that this ensures that elements outside of $\\mathcal{X}$ have no effect on the information-structure of the variable.\n",
    "\n",
    "Typically the lower form is what is seen most commonly, although they are identical. This perspective defines entropy as a measure of, on average, how surprising is every individual possible outcome of $X$. Conviently, entropy is *always positive.* In general, if you have gotten a negative value of $H(X)$, you have done something terribly wrong. Negative information values (almost) never make sense.\n",
    "\n",
    "2) **Entropy as the average number of yes/no quesitons needed to determine the value of a variable.** The intuition for this is reasonably clear. Imagine some variable $X$ with the following probability distribution: $P(X) := \\{P(X=x_1)=1/2, P(X=x_2)=1/4, P(X=x_3)=1/4\\}$. If you were playing a game where you knew that distribution, but not the actual value of $X$ and wanted to guess the state of $X$ using the fewest number of yes/no quesitons, how would you play? \n",
    "\n",
    "You'd start by asking \"is $X=x_1$?\" Half the time, you'd be right, and the game would be over. \n",
    "\n",
    "If it's not $x_1$, then the obvious next question to ask is \"is $X=x_2$?\" After all, if it's not $x_1$, then it's just as likely to be $x_2$ as it is to be $x_3$. \n",
    "\n",
    "If it turns out you're wrong about it being $x_2$, you automatically know it's $x_3$ and the game is over. If we calculate the average number of questions asked, it works out to be:\n",
    "\n",
    "\\begin{equation}\n",
    "H(X) = \\frac{1}{2}(1) + \\frac{1}{2}(2) = 1.5\\text{ bit}\n",
    "\\end{equation}\n",
    "\n",
    "If we plug the distribution above into $-\\sum (P(X)\\log(P(X))$ above, we get the same number out.\n",
    "\n",
    "3) **Entropy as uncertainty about the state of a variable.** Finally, the intuition most commonly used in Complex Systems is that entropy quantifies how uncertain we about the state of the variable (or system) under study. As Feldman notes, Shannon's entropy function satisfies a number of intuitions. Namely that: when the state of the system is known with certainty, the entropy should be $0$ (i.e. there is no uncertainty) and when the probability distribution of states is flat, entropy is maximal (i.e. we have no reason to expect any state over any other). Furthermore, we would like the entropy is invariant under different groupings of states (again, see Feldman 1.2.1 for an example), and that the function itself is continous (no weird asymptotes, small changes in $P(X)$ lead onlly to small changes in $H(X)$).\n",
    "\n",
    "You can prove (although we won't here) that the *only* function that satisfies those four criteria is $H(X) = -\\sum P(X)\\log(P(X))$.\n",
    "\n",
    "### Joint Entropy \n",
    "\n",
    "As with probabilities, we can quantify how much uncertainty is contained within two variables, and like the other information theory measures, it is typically given as the expectation over a distribution of (joint) states (although the other two interpretations are also equally valid).\n",
    "\n",
    "\\begin{equation}\n",
    "H(X,Y) = \\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}}P(X=x, Y=y)\\log(P(X=x,Y=y))\n",
    "\\end{equation}\n",
    "\n",
    "The joint entropy generalizes to multivariate systems quite nicely:\n",
    "\n",
    "\\begin{equation}\n",
    "H(X_1,...X_n) = -\\sum_{x_1\\in\\mathcal{X_1}}...\\sum_{x_n\\in\\mathcal{X_n}}P(x_1...x_n)\\log(P(x_1...x_n))\n",
    "\\end{equation}\n",
    "\n",
    "(Note we have compactified the notation here, which we will do from here on out). \n",
    "You can also look at the joint states of groups of variables, in which case we often concatenate them without commas. For example, for four variables, we might be interested in the joint states of {X,Y} and {A,B}, which we would write $H(XY,AB)$.\n",
    "\n",
    "We have already briefly explored constructing joint probability spaces in the last Notebook, but will revisit the essential logic here, again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(Ch0,Ch1) = 1.975\n"
     ]
    }
   ],
   "source": [
    "#These have already been defined above - showing it again just for completeness sake. \n",
    "C_joint = Counter(list(zip(ch0, ch1)))\n",
    "probs_joint = {key : C_joint[key] / sum(C_joint.values()) for key in C_joint.keys()}\n",
    "\n",
    "H_joint = -1*sum((probs_joint[key]*np.log2(probs_joint[key]) for key in probs_joint.keys()))\n",
    "print(\"H(Ch0,Ch1) =\", round(H_joint, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that the joint entropy is slightly lower than what we would expect if both channels were totally random (i.e. $P(Ch 0 = 0) = P(Ch 0 = 1) = 1/2$) and both were independent. This highlights a nice property of joint entropy: the joint entropy will always be *less* that the sum of the individual entropies togther, but greater than any single consituent entropy considered alone. \n",
    "\n",
    "The logic is the same if we include more channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(0, 0, 0, 0): 180,\n",
       "         (0, 0, 0, 1): 48,\n",
       "         (0, 0, 1, 0): 76,\n",
       "         (0, 0, 1, 1): 30,\n",
       "         (0, 1, 0, 0): 105,\n",
       "         (0, 1, 0, 1): 76,\n",
       "         (0, 1, 1, 0): 18,\n",
       "         (0, 1, 1, 1): 30,\n",
       "         (1, 0, 0, 0): 47,\n",
       "         (1, 0, 0, 1): 23,\n",
       "         (1, 0, 1, 0): 100,\n",
       "         (1, 0, 1, 1): 96,\n",
       "         (1, 1, 0, 0): 32,\n",
       "         (1, 1, 0, 1): 90,\n",
       "         (1, 1, 1, 0): 38,\n",
       "         (1, 1, 1, 1): 211})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch2, ch3 = discrete[2], discrete[3]\n",
    "C_joint = Counter(list(zip(ch0, ch1, ch2, ch3)))\n",
    "C_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(Ch0,Ch1,Ch2,Ch3) = 3.281\n"
     ]
    }
   ],
   "source": [
    "probs_joint = {key : C_joint[key] / sum(C_joint.values()) for key in C_joint.keys()}\n",
    "\n",
    "H_joint = -1*sum({probs_joint[key]*np.log2(probs_joint[key]) for key in probs_joint.keys()})\n",
    "print(\"H(Ch0,Ch1,Ch2,Ch3) =\", round(H_joint, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Entropy\n",
    "\n",
    "Conditional entropy is the uncertainty you have about a variable $X$, after accounting for the states of variable $Y$. It is very important that you remember that entropy is *uncertainty* - you can think of the conditional entropy as the uncertainty in $X$ that is intrinsic to it, or not explicable using information about the state of $Y$.\n",
    "\n",
    "Typically denoted as:\n",
    "\n",
    "\\begin{equation}\n",
    "H(X | Y) = -\\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}}P(X,Y)\\log(P(X|Y))\n",
    "\\end{equation}\n",
    "\n",
    "This formula may look a little surprising (we are used to things for the form $P(X)f(P(X))$, but here we have a conditional probability on the inside of the log and and joint probability outside of it). This has to do with needing to condition all possible states of $X$ on all possible states of $Y$. If we are only interested in one possible state of $Y$, then the restricted conditional entropy is\n",
    "\n",
    "\\begin{equation}\n",
    "H(X | Y=y) = -\\sum_{x\\in\\mathcal{X}}P(X|Y=y)\\log(X|Y=y)\n",
    "\\end{equation}\n",
    "\n",
    "Which is in the more familiar shape of an entropy measure. Unfortunately, this only accounts for the effect of a specific state $y$. If we want information from all possible states of $Y$, we need to average over the distribution of $Y$'s states, which becomes:\n",
    "\n",
    "\\begin{equation}\n",
    "H(X | Y) = \\sum_{y\\in\\mathcal{Y}}P(Y=y)H(X|Y=y)\n",
    "\\end{equation}\n",
    "\n",
    "What we have here, then is nested expectations: the expected entropy of $X$ given some static $Y=y$, and then the expected value of *that* over all possible static $Y=y$ states. This also explains why the equation immediatley above doesn't have the negative sign in front of it: that negative is \"hidden\" in $H(X|Y=y)$.\n",
    "\n",
    "When you work out the math, you get the original formulation given above. \n",
    "\n",
    "Conditional entropy can also be written in terms of joint and individual entropies:\n",
    "\n",
    "\\begin{equation}\n",
    "H(X|Y) = H(X,Y) - H(Y)\n",
    "\\end{equation}\n",
    "\n",
    "Intuitively, this makes sense: the uncertainty about $X$ conditioned on $Y$ is the amount of uncertainty *left over* when you remove the uncertainty from $Y$ from the joint states of both. Again, it is important to remember that entropy is a measure of *uncertainty* NOT *information.* There is a tendency for students to read $H(X|Y)$ as \"the information that $Y$ gives me about $X$. This is NOT the case. It is the uncertainty about $X$ left over when information about the state $Y$ is accounted for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9972551344471869\n",
      "1.0\n",
      "H(Ch0 | Ch1) =  0.9749626957207407\n",
      "H(Ch1 | Ch0) =  0.9777075612735537\n"
     ]
    }
   ],
   "source": [
    "#We can easily calculate the entropy of Ch. 0 given Ch. 1 and vice versa:\n",
    "C0 = Counter(ch0)\n",
    "C1 = Counter(ch1)\n",
    "\n",
    "probs_0 = {key : C0[key] / sum(C0.values()) for key in C0.keys()}\n",
    "probs_1 = {key : C1[key] / sum(C1.values()) for key in C1.keys()}\n",
    "\n",
    "C_joint = C_joint = Counter(list(zip(ch0, ch1))) #Again, this is already definiedm re-doing for completeness. \n",
    "probs_joint = {key : C_joint[key] / sum(C_joint.values()) for key in C_joint.keys()}\n",
    "\n",
    "H_0 = -1*sum((probs_0[key]*np.log2(probs_0[key]) for key in probs_0.keys()))\n",
    "H_1 = -1*sum((probs_1[key]*np.log2(probs_1[key]) for key in probs_1.keys()))\n",
    "\n",
    "print(H_0)\n",
    "print(H_1)\n",
    "\n",
    "H_joint = -sum((probs_joint[key]*np.log2(probs_joint[key]) for key in probs_joint.keys()))\n",
    "\n",
    "print(\"H(Ch0 | Ch1) = \", H_joint - H_1)\n",
    "print(\"H(Ch1 | Ch0) = \", H_joint - H_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these are *not* the same. The conditional entropy function is not symmetrical. Also the condtional probability $H(X|Y)$ will always be smaller than, or equal to $H(X)$, with equality if and only if $X$ and $Y$ are independenet. There is no circumstance in which knowing more about one variable decreases you ability to make predictions about the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Entropy (Kullback-Leibler Divergence)\n",
    "\n",
    "The relative entropy (or KLD) is a measure of how different two probability distributions (typically noted $P(X)$ and $Q(X)$).\n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}(P || Q) = P(X)\\log\\big(\\frac{P(X)}{Q(X)}\\big)\n",
    "\\end{equation}\n",
    "\n",
    "In the context of Bayesian inference, $D_{KL}(P||Q)$ gives the amount of information (or predictive power) gained when one updates one's prior beliefs ($Q(X)$) to the posterior distribution ($P(X)$). Alternately, it can be thought of as the amount of information lost when $Q(X)$ is used to approximate $P(X)$. \n",
    "\n",
    "Consider two distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCwAAAFcCAYAAAADCC/2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "px = np.random.randint(1,101,5)\n",
    "px = px / np.sum(px)\n",
    "qx = np.repeat(1/5, 5)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.bar(np.arange(5), px)\n",
    "plt.ylim([0,1])\n",
    "plt.title(\"P(X)\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.bar(np.arange(5), qx)\n",
    "plt.ylim([0,1])\n",
    "plt.title(\"Q(X)\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.bar(np.arange(5), np.log2(px/qx))\n",
    "plt.title(\"log(P(X)/Q(X))\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Local KLD\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the element-wise differences between $Q(X=x)$ and $P(X=x)$.\n",
    "\n",
    "Working with the Shannon information content, we get $[-\\log(Q(X=x))] - [-\\log(P(X=x))]$. We subtract P(X) from Q(X) to get the \"error\" between the two distributions: how far is distribution $Q$ from distribution $P$? \n",
    "\n",
    "which can be factored to $-1(\\log(Q(X=x)) - \\log(P(X=x))$. \n",
    "\n",
    "The properties of the logarithm allow us to rewrite as $-1\\log\\big(\\frac{(Q(X=x))}{(P(X=x))}\\big)$\n",
    "\n",
    "Finally, we rewrite as $\\log\\big(\\frac{(P(X=x))}{(Q(X=x))}\\big)$\n",
    "\n",
    "Like other entropy measures, the DKL is an expectation value, over $P(X)$ (\"the posteriror\" in this context).\n",
    "\n",
    "One commonly-seen interpretation of the DKL is that it defines some kind of \"distance\" from $Q(X)$ to $P(X)$, however this is innapropriate, as the DKL doesn't define a true distance metric. It is true that $D_{KL}(P || Q) = 0$ if and only if $P(X) = Q(X)$, the measure itself is often non-symmetrical (i.e. $D_{KL}(P||Q) \\not= D_{KL}(Q||P)$! *This means that it is very important that you select your reference distribution with care.*\n",
    "\n",
    "Furthermore, there are restrictions on which distributions are valid entries for DKL. Specifically, $Q(X=x)$ can only equal $0$ if and only if $P(X=x)$ *also equals zero*, otherwise it is undefined (when both are zero, $x \\not\\in\\mathcal{X}$ and so the term is ignored, or considered to be equal to $0$).\n",
    "\n",
    "For a good video building intuition with a simple example (from a source-coding perspective, see: https://www.youtube.com/watch?v=LJwtEaP2xKA)\n",
    "\n",
    "##### Jensen-Shannon Divergence\n",
    "\n",
    "If, for whatever reason, you want a symmetric divergence between two probability distributions, the Jensen-Shannon divergence ($D_{JS}$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "D_{JS}(P||Q) = \\frac{D_{KL}(P||M)+D_{KL}(Q||M)}{2}\n",
    "\\end{equation}\n",
    "\n",
    "where $M$ is the *mixture distribuiton* of $P(X)$ and $Q(X)$,\n",
    "\n",
    "\\begin{equation}\n",
    "M=\\frac{P + Q}{2}\n",
    "\\end{equation}\n",
    "\n",
    "From these moving parts (Shannon entropy, joint entropy, conditional entropy, and KLD), it is possible to build up essentailly every subsequent bit of information theory we will cover going forward, up until we get to partial-information decomposition at the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Entropy \n",
    "\n",
    "Residual entropy ($R(X)$) is a less-often used measure that (in my opinion) is extremely powerful and deserves more attention. It is also useful as a demonstration of how the various \"atomic\" entropies (Shannon, joint, condition) can be used to construct more complex measures that can provide deep insights into the structure of complex systems. \n",
    "\n",
    "The residual entropy is a measure of *how much uncertainty is intrinsic to a single element in a complex system.* For a multivariate system of interacting elements $\\textbf{X} = \\{X_1,X_2,...X_n\\}$\n",
    "\n",
    "\\begin{equation}\n",
    "R(X_i) = H(X_i | X^{-i}) \n",
    "\\end{equation}\n",
    "\n",
    "Where $X^{-i} = \\{X_1, X_2,...X_{i-1}, X_{i+1},...X_n\\}$: that is to say, it is the joint-states of every element of $\\textbf{X}$ *other than $X_i$*. The residual entropy is then understood as the uncertainty in $X_i$ that cannot be resolved by observing any other elements (or combinations of elements) in the system $\\textbf{X}$.\n",
    "\n",
    "To calculate $R(X_i)$, recall that the conditional information can be decomposed into the joint and individual entropies of it's constituents. In this case, this will be two joint entropies: \n",
    "\n",
    "\\begin{equation}\n",
    "H(X_i|X^{-i}) = H(X_i,X^{-i}) - H(X^{-i}) \n",
    "\\end{equation}\n",
    "\n",
    "Obviously, $H(X_i,X^{-i}) = H(\\textbf{X})$ and $H(X^{-i})$ is the joint-entropy of every element excluding $X_i$.\n",
    "\n",
    "We can construct and calculate a simple example using the four channels selected earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual entropy of Ch.0: 0.79908896364954\n",
      "Shannon entropy of Ch. 0 0.9972551344471869\n"
     ]
    }
   ],
   "source": [
    "ch0, ch1, ch2, ch3 = discrete[0], discrete[1], discrete[2], discrete[3]\n",
    "C_full = Counter(zip(ch0, ch1, ch2, ch3))\n",
    "probs_full = {key:C_full[key] / sum(C_full.values()) for key in C_full.keys()}\n",
    "\n",
    "#Suppose we want R(ch0) - we need the joint entropy of Ch. 1, 2, and 3.\n",
    "\n",
    "C_123 = Counter(zip(ch1, ch2, ch3))\n",
    "probs_123 = {key:C_123[key] / sum(C_123.values()) for key in C_123.keys()}\n",
    "\n",
    "H_full = -sum((probs_full[key]*np.log2(probs_full[key]) for key in probs_full.keys()))\n",
    "H_123 = -sum((probs_123[key]*np.log2(probs_123[key]) for key in probs_123.keys()))\n",
    "\n",
    "print(\"Residual entropy of Ch.0:\", H_full - H_123)\n",
    "print(\"Shannon entropy of Ch. 0\", H_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are $\\approx 0.8\\text{ bit}$ of structure in Ch. 0 that cannot be extracted by observing Ch. 1-3. That's about $80\\%$ of the entropy of the channel itself, which suggests that it is not super-constrainted by these three other elements.\n",
    "\n",
    "### Scipy.Stats' Entropy Function\n",
    "\n",
    "This is the last thing. Up until now, I've been calculate entropies \"manually\" using tuple and list comprehensions, so that you can see the structure of $P\\times\\log(P)$. You don't have to do this however: the Scipy.stats package has an entropy function that you can call on distributions (it will also do the KL-divergence if you feed it two disributions of equal length).\n",
    "\n",
    "Read the docs here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html\n",
    "\n",
    "One thing to do aware of is that the default base for all the logarithms is $e$ (which outputs results in units of \"nats\", which are useful when combing information theory and thermodynamics). If you want your result in bits, make sure you specify the argument \"base=2\".\n",
    "\n",
    "If you feed it an unnormalized distribution (i.e. the values don't all sum to 1), it will normalize it for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (unnormed) [10 10 10 10 10 10 10 10 10 10]\n",
      "Entropy Function, Unnormed: 3.322 bit\n",
      " \n",
      "X_norm [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "Entropy Function, Normed 3.322 bit\n",
      " \n",
      "Entropy of X_norm, Default Base 2.303 nat\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "X = np.repeat(10, 10) #A vector of 10 10s. This is *not* a probability distribution, and should be normalized. \n",
    "#Scipy.stats entropy function doesn't care.\n",
    "print(\"X (unnormed)\", X)\n",
    "print(\"Entropy Function, Unnormed:\", round(entropy(X, base=2),3), \"bit\")\n",
    "print(\" \")\n",
    "#If we normalize it, we get the maximum entropy distribution for a 10-state system.\n",
    "X_norm = X / np.sum(X)\n",
    "print(\"X_norm\", X_norm)\n",
    "print(\"Entropy Function, Normed\", round(entropy(X_norm, base=2),3), \"bit\")\n",
    "print(\" \")\n",
    "#Finally, note that if you forget to specify the base, the numbers can be surprising. \n",
    "print(\"Entropy of X_norm, Default Base\", round(entropy(X_norm),3), \"nat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you are doing something that involves many calls to the entropy function and you forget to specify one of the bases, you can end up with bizzare things like $H(X|Y) > H(X)$. Be extra-careful with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
